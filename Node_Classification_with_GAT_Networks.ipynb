{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radu93/graph_learning/blob/main/Node_Classification_with_GAT_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmP1VeISqWgq"
      },
      "source": [
        "# Node Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZYFAlROqWgt"
      },
      "source": [
        "## Initial imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cffnj6s4qWgv",
        "outputId": "f1e01791-006d-432d-fe9e-f66e09fdca3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 32.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 31.2 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BXeb07WqWgw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMizvki6qWgy"
      },
      "source": [
        "## Implementing the forward method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMuw4MCjqWgz"
      },
      "source": [
        "### Linear Transformation\n",
        "\n",
        "Consider the node $i$ with feature vector $\\bar{h}_i\\in\\mathbb R^{F}$. We apply a linear transformation to get a new feature vector $\\bar{h'}_i \\in \\mathbb{R}^{F'}$:\n",
        "$$\n",
        "\\bar{h'}_i = \\textbf{W}\\cdot \\bar{h}_i\n",
        "$$\n",
        "with $\\textbf{W}\\in\\mathbb R^{F'\\times F}$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is a sample linear transformation applied over *nb_nodes*."
      ],
      "metadata": {
        "id": "IfjnjQmhO5Ct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNd7WyPeqWg0",
        "outputId": "1e1cd361-23ed-42d6-ac66-deb8d0ba1a5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.4490, -0.4999],\n",
            "        [ 1.0120, -0.7833],\n",
            "        [ 0.5757,  1.1265],\n",
            "        [ 0.2161, -0.8341],\n",
            "        [ 1.2974, -0.3723]], requires_grad=True)\n",
            "tensor([[0.0161, 0.4029, 0.8167, 0.1829, 0.5342],\n",
            "        [0.6370, 0.5044, 0.1990, 0.0086, 0.0902],\n",
            "        [0.4763, 0.8376, 0.4879, 0.8873, 0.1465]])\n",
            "torch.Size([3, 2])\n"
          ]
        }
      ],
      "source": [
        "in_features = 5\n",
        "out_features = 2\n",
        "nb_nodes = 3\n",
        "\n",
        "W = nn.Parameter(torch.zeros(size=(in_features, out_features))) # xavier paramiter inizializator\n",
        "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
        "\n",
        "print(W)\n",
        "\n",
        "input = torch.rand(nb_nodes, in_features) \n",
        "\n",
        "print(input)\n",
        "\n",
        "# linear transformation\n",
        "h = torch.mm(input, W)\n",
        "N = h.size()[0]\n",
        "\n",
        "print(h.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52XMUZ3qWg0"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an exemplification of the attention mechanism. Its purpose is to add learnable weights between neighboring nodes."
      ],
      "metadata": {
        "id": "O-Zt0EQJP1FL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bokWXcVqWg0"
      },
      "source": [
        "![title](https://github.com/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial3/AttentionMechanism.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the attention weights vector $a\\in\\mathbb{R}^{2F'}$. This vector acts like a global filter of learnable weights between neighboring nodes."
      ],
      "metadata": {
        "id": "pzUeNirBbNBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMUdyemyqWg1",
        "outputId": "5c7816fd-6bce-4d9d-c105-53d077bfdc44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1])\n",
            "Parameter containing:\n",
            "tensor([[-1.1643],\n",
            "        [-0.9359],\n",
            "        [ 1.2391],\n",
            "        [ 1.1750]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "a = nn.Parameter(torch.zeros(size=(2*out_features, 1))) # xavier paramiter inizializator\n",
        "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
        "print(a.shape)\n",
        "print(a)\n",
        "\n",
        "leakyrelu = nn.LeakyReLU(0.2)  # LeakyReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute $a_{input}\\in\\mathbb R^{N \\times N \\times 2F'}$, where $N$ is the number of nodes in the graph. It encodes all possible pairs of nodes together with their concatenated feature vectors."
      ],
      "metadata": {
        "id": "BQdeB0k-aM0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxM6Ms73qWg1",
        "outputId": "eacb47c0-4dd4-4117-e82a-6f844312feba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.6033,  0.2450,  1.6033,  0.2450],\n",
            "         [ 1.6033,  0.2450,  0.4578, -0.5301],\n",
            "         [ 1.6033,  0.2450,  1.2966, -1.1393]],\n",
            "\n",
            "        [[ 0.4578, -0.5301,  1.6033,  0.2450],\n",
            "         [ 0.4578, -0.5301,  0.4578, -0.5301],\n",
            "         [ 0.4578, -0.5301,  1.2966, -1.1393]],\n",
            "\n",
            "        [[ 1.2966, -1.1393,  1.6033,  0.2450],\n",
            "         [ 1.2966, -1.1393,  0.4578, -0.5301],\n",
            "         [ 1.2966, -1.1393,  1.2966, -1.1393]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features)\n",
        "\n",
        "print(a_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyPhZIwwqWg1"
      },
      "source": [
        "![title](https://github.com/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial3/a_input.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The multiplication $a_{input} \\times a$ encodes the application of the filter $a$ over all the nodes. Note that for this exemplification we did not consider any adjacency matrix, so each node is connected with every other node (similar to a complete graph).\n",
        "The result is passed through a LeakyReLU and stored in $e\\in\\mathbb{R}^{N \\times N}$."
      ],
      "metadata": {
        "id": "B_NjthrJbDb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5TxV8QJqWg1"
      },
      "outputs": [],
      "source": [
        "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZSSDRniqWg2",
        "outputId": "d7741645-513d-4f21-d538-6edaad86ba68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 4]) torch.Size([4, 1])\n",
            "\n",
            "torch.Size([3, 3, 1])\n",
            "\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "print(a_input.shape, a.shape)\n",
        "print(\"\")\n",
        "print(torch.matmul(a_input, a).shape)\n",
        "print(\"\")\n",
        "print(torch.matmul(a_input, a).squeeze(2).shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(e)\n",
        "print(e.shape)"
      ],
      "metadata": {
        "id": "Zw77XSsgB0kS",
        "outputId": "83c08004-085e-438b-8b0b-24cbd73c6c09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1786, -0.4303, -0.3656],\n",
            "        [ 2.2377, -0.0185,  0.2311],\n",
            "        [ 1.8312, -0.0998, -0.0351]], grad_fn=<LeakyReluBackward0>)\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3gM4yo8qWg2"
      },
      "source": [
        "### Masked Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now consider a random adjacency matrix, in order to get closer to a real graph."
      ],
      "metadata": {
        "id": "y-JQO2kNdVOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V-paE7dqWg2",
        "outputId": "21568cc7-bd26-442c-b801-3f7aa280db47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adj:\n",
            "tensor([[1, 0, 1],\n",
            "        [0, 1, 0],\n",
            "        [1, 1, 1]])\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "# Masked Attention\n",
        "adj = torch.randint(2, (3, 3))\n",
        "\n",
        "print('adj:')\n",
        "print(adj)\n",
        "\n",
        "zero_vec  = -9e15 * torch.ones_like(e)\n",
        "print(zero_vec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous result $e\\in\\mathbb{R}^{N \\times N}$ includes every possible edge between $N$ graph nodes. We select from $e$ only the edges specified by the adjacency matrix."
      ],
      "metadata": {
        "id": "6U0igWFgd5yR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqYgi46lqWg2",
        "outputId": "3a92c704-78cf-4210-b827-001b927d2e92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 1],\n",
            "        [0, 1, 0],\n",
            "        [1, 1, 1]]) \n",
            " tensor([[ 0.1786, -0.4303, -0.3656],\n",
            "        [ 2.2377, -0.0185,  0.2311],\n",
            "        [ 1.8312, -0.0998, -0.0351]], grad_fn=<LeakyReluBackward0>) \n",
            " tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
            "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
            "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n",
            "attention:\n",
            " tensor([[ 1.7862e-01, -9.0000e+15, -3.6559e-01],\n",
            "        [-9.0000e+15, -1.8499e-02, -9.0000e+15],\n",
            "        [ 1.8312e+00, -9.9797e-02, -3.5074e-02]], grad_fn=<SWhereBackward0>)\n",
            "attention shape:\n",
            " torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "attention = torch.where(adj > 0, e, zero_vec)\n",
        "print(adj,\"\\n\",e,\"\\n\",zero_vec)\n",
        "print('attention:\\n', attention)\n",
        "print('attention shape:\\n', attention.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, pass the result through a softmax layer."
      ],
      "metadata": {
        "id": "Ik4pOn-dfKIC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXFQsCUJqWg3"
      },
      "outputs": [],
      "source": [
        "attention = F.softmax(attention, dim=1)\n",
        "h_prime   = torch.matmul(attention, h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAPZ1Yi1qWg3",
        "outputId": "750b9d21-9e7c-41d6-a806-dca68ea8bd2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6328, 0.0000, 0.3672],\n",
              "        [0.0000, 1.0000, 0.0000],\n",
              "        [0.7694, 0.1116, 0.1190]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnv5GP1GqWg3",
        "outputId": "9a26d4f8-8697-4cef-cb06-308eb2ec7767",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.4907, -0.2633],\n",
              "        [ 0.4578, -0.5301],\n",
              "        [ 1.4390, -0.0062]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "h_prime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoP5H6ZbqWg4"
      },
      "source": [
        "This is the effect of applying an attention filter over node features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ26OPwOqWg4",
        "outputId": "afdadc61-0931-4be7-b2f0-a413cf9910fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.4907, -0.2633],\n",
            "        [ 0.4578, -0.5301],\n",
            "        [ 1.4390, -0.0062]], grad_fn=<MmBackward0>) \n",
            " tensor([[ 1.6033,  0.2450],\n",
            "        [ 0.4578, -0.5301],\n",
            "        [ 1.2966, -1.1393]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(h_prime,\"\\n\",h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIxoAAdqWg4"
      },
      "source": [
        "# Building the Attentional Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part puts together all the building blocks and implements a Graph Attentional Layer as a pytorch Module."
      ],
      "metadata": {
        "id": "gXujvIPdgwiy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1dxLV-dqWg5"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.dropout       = dropout        # drop prob = 0.6\n",
        "        self.in_features   = in_features    # \n",
        "        self.out_features  = out_features   # \n",
        "        self.alpha         = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n",
        "        self.concat        = concat         # conacat = True for all layers except the output layer.\n",
        "\n",
        "        \n",
        "        # Xavier Initialization of Weights\n",
        "        # Alternatively use weights_init to apply weights of choice \n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        \n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "        \n",
        "        # LeakyReLU\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # Linear Transformation\n",
        "        h = torch.mm(input, self.W) # matrix multiplication\n",
        "        N = h.size()[0]\n",
        "\n",
        "        # Attention Mechanism\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        # Masked Attention\n",
        "        zero_vec  = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        \n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime   = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U1XQXedqWg5"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cora Dataset\n",
        "\n",
        "We will use **Cora**, a citation graph containing 2708 scientific publications. For each publication there is a 1433-dimensional feature vector, which is a bag-of-words representation (with a small, fixed dictionary) of the paper text. The edges in this graph represent citations, and are commonly treated as undirected. The goal is to classify each paper into one of seven classes (topics).\n",
        "\n",
        "Load and inspect the Cora dataset using the PyTorch Geometric library:"
      ],
      "metadata": {
        "id": "LLjG7jP9hbVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5jwISJ0qWg5",
        "outputId": "2c4ed1e3-4eca-489a-ff98-20956f762d57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs: 1\n",
            "Number of features: 1433\n",
            "Number of classes: 7\n",
            "\n",
            "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
            "Number of nodes: 2708\n",
            "Number of edges: 10556\n",
            "Average node degree: 3.90\n",
            "Number of training nodes: 140\n",
            "Number of validation nodes: 500\n",
            "Number of test nodes: 1000\n",
            "\n",
            "Train mask:\n",
            "tensor([ True,  True,  True,  ..., False, False, False])\n",
            "Edge index:\n",
            "tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
            "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])\n",
            "\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "name_data = 'Cora'\n",
        "dataset = Planetoid(root= '/tmp/' + name_data, name = name_data)\n",
        "dataset.transform = T.NormalizeFeatures()\n",
        "\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "print()\n",
        "\n",
        "g0_data = dataset[0]\n",
        "print(g0_data)\n",
        "print(f'Number of nodes: {g0_data.num_nodes}')\n",
        "print(f'Number of edges: {g0_data.num_edges}')\n",
        "print(f'Average node degree: {g0_data.num_edges / g0_data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {g0_data.train_mask.sum()}')\n",
        "print(f'Number of validation nodes: {g0_data.val_mask.sum()}')\n",
        "print(f'Number of test nodes: {g0_data.test_mask.sum()}')\n",
        "print()\n",
        "\n",
        "print('Train mask:')\n",
        "print(g0_data.train_mask)\n",
        "print('Edge index:')\n",
        "print(g0_data.edge_index)\n",
        "print()\n",
        "\n",
        "# Some more utility functions on a Data object \n",
        "print(f'Contains isolated nodes: {g0_data.has_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {g0_data.has_self_loops()}')\n",
        "print(f'Is undirected: {g0_data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAT Network"
      ],
      "metadata": {
        "id": "aC9nVmoGjPmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a GAT Network with 2 Attentional Layers.\n",
        "\n",
        "The first layer uses a multi-head attention. This means it applies the attentional convolution multiple times and stacks the results. The dimension of its output features is $8 \\times 8$ for each node (8 heads with 8 output features each).\n",
        "\n",
        "The second layer gets the linearized 64 features and outputs 7 features."
      ],
      "metadata": {
        "id": "H6cTQgVxjXLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the first test, we use the GATConv layer offered by pytorch."
      ],
      "metadata": {
        "id": "bgbx2HqClSj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GAT, self).__init__()\n",
        "        self.hid = 8\n",
        "        self.in_head = 8\n",
        "        self.out_head = 1\n",
        "        \n",
        "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
        "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,\n",
        "                             heads=self.out_head, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "                \n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "Y4jj5n9QlbZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the model using and Adam optimizer for 1000 epochs. Each epoch feeds the entire Cora graph into the network. The computed loss takes into consideration only the nodes from the training set."
      ],
      "metadata": {
        "id": "i-8e7bZMoI94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFaquvqJqWg5",
        "outputId": "ffd4c2ef-94de-42b5-b5ec-55e880a621c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAT(\n",
            "  (conv1): GATConv(1433, 8, heads=8)\n",
            "  (conv2): GATConv(64, 7, heads=1)\n",
            ")\n",
            "tensor(1.9452, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6751, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5434, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5658, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5649, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = \"cpu\"\n",
        "\n",
        "model = GAT().to(device)\n",
        "print(model)\n",
        "\n",
        "data = dataset[0].to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    \n",
        "    if epoch%200 == 0:\n",
        "        print(loss)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model prediction for the nodes in the testing set. It should get at least 82% accuracy."
      ],
      "metadata": {
        "id": "kt27QBHkpasQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ltm2t6AqWg6",
        "outputId": "28a0c371-498a-4f27-f5b0-593ba0c085f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8220\n",
            "tensor([2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 6, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 6,\n",
            "        2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5,\n",
            "        5, 5, 2, 2, 2, 2, 2, 6, 6, 3, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 6, 0, 0,\n",
            "        3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 4, 4, 4, 4, 4, 3, 2, 5, 5, 5, 5,\n",
            "        6, 5, 5, 5, 5, 6, 4, 4, 0, 0, 1, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0,\n",
            "        0, 0, 0, 0, 3, 4, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        6, 6, 5, 6, 6, 0, 5, 5, 5, 0, 5, 4, 4, 0, 3, 3, 3, 2, 3, 1, 3, 3, 3, 2,\n",
            "        3, 3, 1, 3, 2, 4, 4, 4, 3, 3, 3, 3, 3, 3, 6, 3, 0, 0, 0, 0, 0, 5, 5, 5,\n",
            "        4, 0, 6, 6, 0, 0, 1, 3, 5, 5, 6, 6, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4,\n",
            "        4, 1, 1, 1, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 5, 5, 3, 3, 3, 3, 3,\n",
            "        0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 1, 0, 6, 6, 6, 6, 2, 3, 3,\n",
            "        0, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 0, 6, 0, 6, 6, 0, 0, 3, 3,\n",
            "        3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 2, 6, 3, 0, 0, 0, 0, 6, 6, 6, 6, 6, 3, 3,\n",
            "        6, 6, 6, 2, 2, 1, 0, 0, 0, 5, 5, 3, 3, 6, 5, 0, 0, 0, 0, 0, 5, 5, 0, 4,\n",
            "        6, 0, 6, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 2, 6, 1, 0,\n",
            "        3, 3, 3, 3, 3, 6, 1, 0, 2, 2, 4, 4, 4, 4, 4, 5, 6, 3, 3, 0, 0, 0, 0, 5,\n",
            "        4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 1, 1, 3, 1, 6, 1, 3, 3, 4,\n",
            "        4, 4, 4, 0, 4, 4, 0, 0, 4, 5, 5, 5, 5, 5, 0, 5, 0, 0, 6, 2, 0, 5, 6, 3,\n",
            "        5, 5, 5, 5, 5, 4, 4, 4, 4, 6, 4, 0, 3, 4, 4, 4, 1, 4, 4, 3, 3, 3, 3, 2,\n",
            "        3, 3, 3, 0, 0, 2, 1, 3, 6, 3, 1, 4, 0, 0, 1, 5, 1, 1, 5, 1, 1, 1, 0, 1,\n",
            "        4, 0, 2, 4, 4, 4, 3, 3, 3, 3, 0, 3, 3, 4, 4, 0, 4, 4, 4, 4, 4, 4, 3, 0,\n",
            "        0, 4, 2, 2, 3, 3, 4, 5, 0, 2, 2, 3, 3, 3, 3, 3, 3, 0, 5, 5, 4, 1, 4, 4,\n",
            "        4, 4, 4, 4, 0, 3, 4, 4, 6, 2, 2, 2, 2, 4, 6, 6, 6, 0, 3, 4, 4, 4, 3, 3,\n",
            "        0, 5, 4, 5, 0, 0, 3, 3, 3, 2, 3, 2, 4, 4, 0, 0, 3, 2, 6, 0, 0, 0, 3, 5,\n",
            "        5, 1, 3, 4, 4, 4, 4, 4, 6, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
            "        2, 0, 6, 6, 2, 6, 6, 2, 2, 6, 2, 4, 4, 4, 2, 5, 5, 2, 2, 3, 0, 4, 4, 3,\n",
            "        2, 3, 1, 6, 6, 5, 0, 4, 4, 6, 3, 1, 1, 4, 0, 5, 2, 3, 3, 3, 4, 5, 4, 0,\n",
            "        3, 3, 0, 2, 1, 1, 5, 2, 3, 3, 5, 0, 2, 3, 2, 2, 5, 5, 4, 3, 4, 3, 2, 2,\n",
            "        4, 2, 4, 5, 5, 3, 2, 3, 1, 3, 3, 3, 4, 5, 4, 3, 3, 3, 3, 3, 0, 0, 2, 4,\n",
            "        4, 4, 3, 3, 3, 5, 2, 3, 2, 2, 2, 3, 2, 0, 0, 4, 4, 0, 0, 0, 3, 4, 3, 2,\n",
            "        3, 3, 3, 0, 0, 3, 3, 3, 3, 2, 3, 4, 2, 2, 6, 4, 3, 3, 4, 1, 5, 0, 4, 3,\n",
            "        2, 2, 1, 3, 2, 3, 4, 4, 6, 3, 2, 2, 6, 2, 3, 0, 2, 3, 2, 4, 2, 4, 2, 2,\n",
            "        0, 5, 6, 4, 3, 3, 3, 2, 5, 3, 3, 4, 3, 3, 3, 3, 3, 4, 6, 6, 5, 2, 2, 2,\n",
            "        5, 4, 4, 4, 4, 6, 3, 2, 2, 6, 2, 3, 2, 2, 2, 3, 0, 4, 4, 3, 3, 4, 4, 3,\n",
            "        3, 0, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 3, 3, 2, 3, 2,\n",
            "        6, 3, 4, 4, 4, 3, 3, 3, 3, 3, 0, 3, 2, 1, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('Accuracy: {:.4f}'.format(acc))\n",
        "\n",
        "print(pred[data.test_mask])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smoU-zXdqWg6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "Node Classification with GAT Networks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}