{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radu93/graph_learning/blob/main/Node_Classification_with_GAT_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmP1VeISqWgq"
      },
      "source": [
        "# Node Classification\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZYFAlROqWgt"
      },
      "source": [
        "## Initial imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cffnj6s4qWgv",
        "outputId": "8c9e4adc-db36-492a-bca1-1a2e8a38a217",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.11.0+cu113\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "os.environ['TORCH'] = torch.__version__\n",
        "print(torch.__version__)\n",
        "\n",
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
        "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BXeb07WqWgw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMizvki6qWgy"
      },
      "source": [
        "## Implementing the forward method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMuw4MCjqWgz"
      },
      "source": [
        "### Linear Transformation\n",
        "\n",
        "Consider the node $i$ with feature vector $\\bar{h}_i\\in\\mathbb R^{F}$. We apply a linear transformation to get a new feature vector $\\bar{h'}_i \\in \\mathbb{R}^{F'}$:\n",
        "$$\n",
        "\\bar{h'}_i = \\textbf{W}\\cdot \\bar{h}_i\n",
        "$$\n",
        "with $\\textbf{W}\\in\\mathbb R^{F'\\times F}$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is a sample linear transformation applied over *nb_nodes*."
      ],
      "metadata": {
        "id": "IfjnjQmhO5Ct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNd7WyPeqWg0",
        "outputId": "ef0c2475-6c3d-4278-c64f-f9c9390352f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.7644,  0.8916],\n",
            "        [-0.8917,  0.4089],\n",
            "        [ 0.8180, -1.1785],\n",
            "        [-1.0833,  1.1294],\n",
            "        [ 1.0618,  0.1176]], requires_grad=True)\n",
            "tensor([[6.9035e-01, 1.5097e-01, 3.3485e-01, 4.2646e-02, 2.0596e-01],\n",
            "        [3.5916e-01, 8.5551e-01, 4.5568e-04, 6.8080e-02, 4.2548e-01],\n",
            "        [4.1077e-01, 1.5948e-01, 2.6342e-01, 4.9429e-01, 1.1808e-01]])\n",
            "torch.Size([3, 2])\n"
          ]
        }
      ],
      "source": [
        "in_features = 5\n",
        "out_features = 2\n",
        "nb_nodes = 3\n",
        "\n",
        "W = nn.Parameter(torch.zeros(size=(in_features, out_features))) # xavier paramiter inizializator\n",
        "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
        "\n",
        "print(W)\n",
        "\n",
        "input = torch.rand(nb_nodes, in_features) \n",
        "\n",
        "print(input)\n",
        "\n",
        "# linear transformation\n",
        "h = torch.mm(input, W)\n",
        "N = h.size()[0]\n",
        "\n",
        "print(h.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X52XMUZ3qWg0"
      },
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an exemplification of the attention mechanism. Its purpose is to add learnable weights between neighboring nodes."
      ],
      "metadata": {
        "id": "O-Zt0EQJP1FL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bokWXcVqWg0"
      },
      "source": [
        "![title](https://github.com/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial3/AttentionMechanism.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the attention weights vector $a\\in\\mathbb{R}^{2F'}$. This vector acts like a global filter of learnable weights between neighboring nodes."
      ],
      "metadata": {
        "id": "pzUeNirBbNBa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMUdyemyqWg1",
        "outputId": "7dd0dad3-4095-4250-d525-113a7f595194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1])\n",
            "Parameter containing:\n",
            "tensor([[0.8732],\n",
            "        [0.2180],\n",
            "        [1.3072],\n",
            "        [1.3225]], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "a = nn.Parameter(torch.zeros(size=(2*out_features, 1))) # xavier paramiter inizializator\n",
        "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
        "print(a.shape)\n",
        "print(a)\n",
        "\n",
        "leakyrelu = nn.LeakyReLU(0.2)  # LeakyReLU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute $a_{input}\\in\\mathbb R^{N \\times N \\times 2F'}$, where $N$ is the number of nodes in the graph. It encodes all possible pairs of nodes together with their concatenated feature vectors."
      ],
      "metadata": {
        "id": "BQdeB0k-aM0W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxM6Ms73qWg1",
        "outputId": "bbea5418-3025-4129-b003-c07a5a571602",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.2159,  0.3550, -0.2159,  0.3550],\n",
            "         [-0.2159,  0.3550, -0.6590,  0.7964],\n",
            "         [-0.2159,  0.3550, -0.6508,  0.6931]],\n",
            "\n",
            "        [[-0.6590,  0.7964, -0.2159,  0.3550],\n",
            "         [-0.6590,  0.7964, -0.6590,  0.7964],\n",
            "         [-0.6590,  0.7964, -0.6508,  0.6931]],\n",
            "\n",
            "        [[-0.6508,  0.6931, -0.2159,  0.3550],\n",
            "         [-0.6508,  0.6931, -0.6590,  0.7964],\n",
            "         [-0.6508,  0.6931, -0.6508,  0.6931]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features)\n",
        "\n",
        "print(a_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyPhZIwwqWg1"
      },
      "source": [
        "![title](https://github.com/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial3/a_input.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The multiplication $a_{input} \\times a$ encodes the application of the filter $a$ over all the nodes. Note that for this exemplification we did not consider any adjacency matrix, so each node is connected with every other node (similar to a complete graph).\n",
        "The result is passed through a LeakyReLU and stored in $e\\in\\mathbb{R}^{N \\times N}$."
      ],
      "metadata": {
        "id": "B_NjthrJbDb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5TxV8QJqWg1"
      },
      "outputs": [],
      "source": [
        "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZSSDRniqWg2",
        "outputId": "eb2f7e74-1573-40f8-9947-608a8ad464c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 3, 4]) torch.Size([4, 1])\n",
            "\n",
            "torch.Size([3, 3, 1])\n",
            "\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "print(a_input.shape, a.shape)\n",
        "print(\"\")\n",
        "print(torch.matmul(a_input, a).shape)\n",
        "print(\"\")\n",
        "print(torch.matmul(a_input, a).squeeze(2).shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(e)\n",
        "print(e.shape)"
      ],
      "metadata": {
        "id": "Zw77XSsgB0kS",
        "outputId": "4c26dbcf-3902-4853-9ba7-7d73103389f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0760,  0.0805, -0.0090],\n",
            "        [-0.0429, -0.0420, -0.0672],\n",
            "        [-0.0460, -0.0451, -0.0702]], grad_fn=<LeakyReluBackward0>)\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3gM4yo8qWg2"
      },
      "source": [
        "### Masked Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now consider a random adjacency matrix, in order to get closer to a real graph."
      ],
      "metadata": {
        "id": "y-JQO2kNdVOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1V-paE7dqWg2",
        "outputId": "70390db5-7fdb-4131-802c-951cc2f78d7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adj:\n",
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 1],\n",
            "        [1, 0, 0]])\n",
            "torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "# Masked Attention\n",
        "adj = torch.randint(2, (3, 3))\n",
        "\n",
        "print('adj:')\n",
        "print(adj)\n",
        "\n",
        "zero_vec  = -9e15 * torch.ones_like(e)\n",
        "print(zero_vec.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous result $e\\in\\mathbb{R}^{N \\times N}$ includes every possible edge between $N$ graph nodes. We select from $e$ only the edges specified by the adjacency matrix."
      ],
      "metadata": {
        "id": "6U0igWFgd5yR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqYgi46lqWg2",
        "outputId": "005fd15f-0f40-4757-c27a-53d081ea70c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 1],\n",
            "        [1, 0, 0]]) \n",
            " tensor([[ 0.0760,  0.0805, -0.0090],\n",
            "        [-0.0429, -0.0420, -0.0672],\n",
            "        [-0.0460, -0.0451, -0.0702]], grad_fn=<LeakyReluBackward0>) \n",
            " tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
            "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
            "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n",
            "attention:\n",
            " tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
            "        [-9.0000e+15, -9.0000e+15, -6.7187e-02],\n",
            "        [-4.6002e-02, -9.0000e+15, -9.0000e+15]], grad_fn=<SWhereBackward0>)\n",
            "attention shape:\n",
            " torch.Size([3, 3])\n"
          ]
        }
      ],
      "source": [
        "attention = torch.where(adj > 0, e, zero_vec)\n",
        "print(adj,\"\\n\",e,\"\\n\",zero_vec)\n",
        "print('attention:\\n', attention)\n",
        "print('attention shape:\\n', attention.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, pass the result through a softmax layer."
      ],
      "metadata": {
        "id": "Ik4pOn-dfKIC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXFQsCUJqWg3"
      },
      "outputs": [],
      "source": [
        "attention = F.softmax(attention, dim=1)\n",
        "h_prime   = torch.matmul(attention, h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAPZ1Yi1qWg3",
        "outputId": "e717f532-e24a-4499-8602-0b4d221e74ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3333, 0.3333, 0.3333],\n",
              "        [0.0000, 0.0000, 1.0000],\n",
              "        [1.0000, 0.0000, 0.0000]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnv5GP1GqWg3",
        "outputId": "941e2c73-d9de-4263-e53f-2334def55233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5086,  0.6148],\n",
              "        [-0.6508,  0.6931],\n",
              "        [-0.2159,  0.3550]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "h_prime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoP5H6ZbqWg4"
      },
      "source": [
        "This is the effect of applying an attention filter over node features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJ26OPwOqWg4",
        "outputId": "b2d820f3-041b-42da-f7c1-8f0c5daa46d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5086,  0.6148],\n",
            "        [-0.6508,  0.6931],\n",
            "        [-0.2159,  0.3550]], grad_fn=<MmBackward0>) \n",
            " tensor([[-0.2159,  0.3550],\n",
            "        [-0.6590,  0.7964],\n",
            "        [-0.6508,  0.6931]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(h_prime,\"\\n\",h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIxoAAdqWg4"
      },
      "source": [
        "# Building a basic Attentional Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part puts together all the building blocks and implements a Graph Attentional Layer as a PyTorch Module."
      ],
      "metadata": {
        "id": "gXujvIPdgwiy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1dxLV-dqWg5"
      },
      "outputs": [],
      "source": [
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2, concat=True):\n",
        "        super(GATLayer, self).__init__()\n",
        "        self.dropout       = dropout        # drop prob = 0.6\n",
        "        self.in_features   = in_features    # \n",
        "        self.out_features  = out_features   # \n",
        "        self.alpha         = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n",
        "        self.concat        = concat         # conacat = True for all layers except the output layer.\n",
        "\n",
        "        # Xavier Initialization of Weights\n",
        "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        \n",
        "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "        \n",
        "        # LeakyReLU\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        # Linear Transformation\n",
        "        h = torch.mm(input, self.W) # matrix multiplication\n",
        "        N = h.size()[0]\n",
        "\n",
        "        # Attention Mechanism\n",
        "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
        "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
        "\n",
        "        # Masked Attention\n",
        "        zero_vec  = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        \n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime   = torch.matmul(attention, h)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U1XQXedqWg5"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cora Dataset\n",
        "\n",
        "We will use **Cora**, a citation graph containing 2708 scientific publications. For each publication there is a 1433-dimensional feature vector, which is a bag-of-words representation (with a small, fixed dictionary) of the paper text. The edges in this graph represent citations, and are commonly treated as undirected. The goal is to classify each paper into one of seven classes (topics).\n",
        "\n",
        "Load and inspect the Cora dataset using the PyTorch Geometric library:"
      ],
      "metadata": {
        "id": "LLjG7jP9hbVj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5jwISJ0qWg5",
        "outputId": "2b689d37-ce6a-4d0c-c7c4-814cc0baa253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of graphs: 1\n",
            "Number of features: 1433\n",
            "Number of classes: 7\n",
            "\n",
            "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
            "Number of nodes: 2708\n",
            "Number of edges: 10556\n",
            "Average node degree: 3.90\n",
            "Number of training nodes: 140\n",
            "Number of validation nodes: 500\n",
            "Number of test nodes: 1000\n",
            "\n",
            "Train mask:\n",
            "tensor([ True,  True,  True,  ..., False, False, False])\n",
            "Edge index:\n",
            "tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
            "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])\n",
            "\n",
            "Contains isolated nodes: False\n",
            "Contains self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "name_data = 'Cora'\n",
        "dataset = Planetoid(root= '/tmp/' + name_data, name = name_data)\n",
        "dataset.transform = T.NormalizeFeatures()\n",
        "\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "print()\n",
        "\n",
        "g0_data = dataset[0]\n",
        "print(g0_data)\n",
        "print(f'Number of nodes: {g0_data.num_nodes}')\n",
        "print(f'Number of edges: {g0_data.num_edges}')\n",
        "print(f'Average node degree: {g0_data.num_edges / g0_data.num_nodes:.2f}')\n",
        "print(f'Number of training nodes: {g0_data.train_mask.sum()}')\n",
        "print(f'Number of validation nodes: {g0_data.val_mask.sum()}')\n",
        "print(f'Number of test nodes: {g0_data.test_mask.sum()}')\n",
        "print()\n",
        "\n",
        "print('Train mask:')\n",
        "print(g0_data.train_mask)\n",
        "print('Edge index:')\n",
        "print(g0_data.edge_index)\n",
        "print()\n",
        "\n",
        "# Some more utility functions on a Data object \n",
        "print(f'Contains isolated nodes: {g0_data.has_isolated_nodes()}')\n",
        "print(f'Contains self-loops: {g0_data.has_self_loops()}')\n",
        "print(f'Is undirected: {g0_data.is_undirected()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAT Network"
      ],
      "metadata": {
        "id": "aC9nVmoGjPmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use a GAT Network with 2 Attentional Layers.\n",
        "\n",
        "The first layer uses a multi-head attention. This means it applies the attentional convolution multiple times and stacks the results. The dimension of its output features is $8 \\times 8$ for each node (8 heads with 8 output features each).\n",
        "\n",
        "The second layer gets the linearized 64 features and outputs 7 features.\n",
        "\n",
        "One of the main advantages of attentional convolution is that the entire graph must not need to be known in advance. However, in this experiment we introduce the entire Cora graph into the model at each training epoch. A possible approach that would leverage the aforementioned property might be to feed the Cora graph into the model node by node during the training phase."
      ],
      "metadata": {
        "id": "H6cTQgVxjXLY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using GATConv Layers\n",
        "\n",
        "For the first test, we use the [`GATConv`](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv) layer offered by PyTorch."
      ],
      "metadata": {
        "id": "bgbx2HqClSj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GAT, self).__init__()\n",
        "        self.hid = 8\n",
        "        self.in_head = 8\n",
        "        self.out_head = 1\n",
        "        \n",
        "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
        "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,\n",
        "                             heads=self.out_head, dropout=0.6)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "                \n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        \n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "Y4jj5n9QlbZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the model using and Adam optimizer for 1000 epochs. Each epoch feeds the entire Cora graph into the network. The computed loss takes into consideration only the nodes from the training set."
      ],
      "metadata": {
        "id": "i-8e7bZMoI94"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFaquvqJqWg5",
        "outputId": "0a49c5df-3dbc-48a2-8b50-aa40e5147c2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GAT(\n",
            "  (conv1): GATConv(1433, 8, heads=8)\n",
            "  (conv2): GATConv(64, 7, heads=1)\n",
            ")\n",
            "tensor(1.9487, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6854, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5262, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5165, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.5837, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = \"cpu\"\n",
        "\n",
        "model = GAT().to(device)\n",
        "print(model)\n",
        "\n",
        "data = dataset[0].to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1000):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    \n",
        "    if epoch%200 == 0:\n",
        "        print(loss)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model prediction for the nodes in the testing set. It should get at least 82% accuracy."
      ],
      "metadata": {
        "id": "kt27QBHkpasQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ltm2t6AqWg6",
        "outputId": "0b3264ce-5209-49fe-d102-c8d476da1efd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8190\n",
            "tensor([2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2,\n",
            "        2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 6, 5, 4, 4, 4, 1, 1, 1, 1, 1, 1, 6,\n",
            "        2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5,\n",
            "        5, 5, 2, 2, 2, 2, 2, 6, 6, 1, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 6, 0, 6,\n",
            "        3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3,\n",
            "        3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 4, 4, 4, 4, 4, 3, 2, 5, 5, 5, 5,\n",
            "        6, 5, 5, 5, 5, 6, 4, 3, 0, 0, 1, 0, 0, 0, 6, 6, 6, 6, 6, 6, 6, 0, 0, 0,\n",
            "        0, 0, 0, 0, 3, 3, 0, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        6, 6, 5, 6, 6, 0, 5, 5, 5, 0, 5, 4, 4, 0, 3, 3, 3, 2, 3, 1, 3, 3, 3, 6,\n",
            "        3, 3, 1, 3, 3, 4, 4, 4, 3, 3, 3, 3, 3, 3, 6, 3, 0, 0, 0, 0, 6, 5, 5, 5,\n",
            "        4, 0, 6, 6, 0, 0, 1, 3, 5, 5, 6, 6, 4, 4, 4, 4, 3, 3, 4, 4, 4, 3, 4, 4,\n",
            "        4, 1, 1, 1, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 5, 5, 3, 3, 3, 3, 3,\n",
            "        0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3,\n",
            "        1, 1, 1, 1, 1, 0, 0, 0, 6, 6, 0, 0, 3, 0, 1, 1, 0, 6, 6, 6, 6, 2, 3, 3,\n",
            "        0, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 4, 0, 6, 0, 6, 6, 0, 0, 3, 3,\n",
            "        3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 2, 6, 3, 0, 6, 0, 0, 6, 6, 6, 6, 6, 3, 3,\n",
            "        6, 6, 6, 2, 2, 1, 0, 0, 0, 5, 5, 3, 3, 6, 5, 0, 0, 0, 0, 0, 5, 5, 0, 4,\n",
            "        6, 6, 6, 4, 6, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 1, 6, 1, 0,\n",
            "        3, 3, 3, 3, 3, 6, 1, 0, 2, 2, 4, 4, 4, 4, 4, 5, 6, 3, 3, 0, 0, 0, 0, 5,\n",
            "        4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 1, 1, 3, 1, 6, 1, 3, 3, 4,\n",
            "        4, 4, 4, 0, 4, 4, 0, 0, 4, 5, 5, 5, 5, 5, 5, 5, 0, 0, 6, 2, 0, 5, 6, 3,\n",
            "        5, 5, 5, 5, 5, 4, 4, 4, 4, 6, 4, 0, 3, 4, 0, 4, 1, 3, 4, 3, 3, 3, 3, 2,\n",
            "        3, 3, 3, 0, 0, 2, 1, 3, 6, 3, 1, 3, 3, 0, 1, 5, 1, 1, 5, 1, 1, 1, 0, 1,\n",
            "        4, 0, 2, 4, 4, 4, 3, 3, 3, 3, 0, 3, 3, 4, 4, 0, 4, 4, 4, 4, 4, 3, 3, 0,\n",
            "        0, 0, 2, 2, 3, 3, 4, 5, 0, 2, 2, 3, 3, 3, 3, 3, 3, 2, 5, 5, 4, 1, 4, 4,\n",
            "        4, 4, 4, 4, 0, 3, 4, 4, 6, 2, 2, 2, 2, 4, 6, 6, 6, 0, 3, 4, 4, 4, 3, 3,\n",
            "        0, 5, 4, 5, 0, 3, 3, 3, 3, 2, 3, 2, 0, 4, 0, 0, 3, 2, 6, 0, 0, 0, 3, 5,\n",
            "        5, 1, 3, 4, 4, 5, 4, 4, 6, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2,\n",
            "        2, 0, 6, 6, 2, 6, 6, 2, 2, 6, 2, 4, 4, 4, 2, 5, 5, 2, 2, 3, 0, 4, 4, 3,\n",
            "        2, 3, 1, 6, 6, 5, 0, 4, 4, 6, 3, 1, 1, 3, 0, 5, 2, 3, 3, 3, 4, 5, 5, 0,\n",
            "        3, 3, 0, 2, 1, 1, 5, 2, 3, 3, 5, 0, 2, 3, 2, 2, 5, 5, 4, 3, 4, 3, 2, 2,\n",
            "        4, 2, 4, 5, 5, 3, 2, 3, 1, 3, 3, 3, 4, 5, 4, 3, 3, 3, 1, 3, 0, 0, 2, 4,\n",
            "        4, 4, 3, 3, 3, 5, 2, 3, 2, 2, 2, 3, 2, 0, 0, 4, 4, 0, 0, 0, 3, 2, 3, 3,\n",
            "        3, 3, 3, 0, 0, 3, 3, 3, 3, 2, 3, 4, 2, 2, 6, 4, 3, 2, 4, 1, 5, 0, 4, 3,\n",
            "        2, 2, 1, 3, 2, 3, 3, 3, 6, 3, 2, 2, 6, 1, 3, 0, 2, 3, 2, 4, 2, 5, 2, 2,\n",
            "        0, 5, 6, 4, 3, 3, 3, 2, 5, 3, 3, 4, 3, 3, 3, 3, 3, 4, 6, 6, 5, 2, 2, 2,\n",
            "        5, 4, 4, 4, 4, 6, 3, 2, 2, 6, 2, 3, 1, 2, 2, 3, 0, 4, 4, 3, 3, 4, 4, 3,\n",
            "        3, 0, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 3, 3, 2, 3, 2,\n",
            "        6, 3, 4, 4, 3, 3, 3, 3, 3, 3, 0, 3, 2, 1, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "_, pred = model(data).max(dim=1)\n",
        "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('Accuracy: {:.4f}'.format(acc))\n",
        "\n",
        "print(pred[data.test_mask])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using our basic Layer"
      ],
      "metadata": {
        "id": "pcdF5U8s81aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this test, we use the GATLayer that we manually implemented above. We use the same architecture for the network."
      ],
      "metadata": {
        "id": "yunzvfkZ9Bk_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smoU-zXdqWg6"
      },
      "outputs": [],
      "source": [
        "class GATBasic(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GATBasic, self).__init__()\n",
        "        self.hid = 8\n",
        "        self.in_head = 8\n",
        "\n",
        "        # multi head attention\n",
        "        self.attentions = [ GATLayer(dataset.num_features, self.hid, concat=True) for _ in range(self.in_head) ]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        # single head attention output layer\n",
        "        self.conv2 = GATLayer(self.hid * self.in_head, dataset.num_classes, concat=False)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "      x = F.dropout(x, p=0.6, training=self.training)\n",
        "      x = torch.cat([ att(x, adj) for att in self.attentions ], dim=1)\n",
        "      x = F.dropout(x, p=0.6, training=self.training)\n",
        "      x = self.conv2(x, adj)\n",
        "      x = F.log_softmax(x, dim=1)\n",
        "\n",
        "      return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unfortunately, our implementation uses extremely large matrix multiplications which are not optimized. Recall that $a_{input}\\in\\mathbb R^{N \\times N \\times 2F'}$ and it includes every pair of nodes, most of which are not linked in the  Cora graph. For this reason, the training is slow compared to the PyTorch GATConv Layers.\n",
        "\n",
        "However, training the model for 500 epochs gets an accuracy of about 80%.\n",
        "\n"
      ],
      "metadata": {
        "id": "lBFPE-QKDeun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import to_dense_adj\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = \"cpu\"\n",
        "\n",
        "model = GATBasic().to(device)\n",
        "print(model)\n",
        "\n",
        "data = dataset[0].to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
        "\n",
        "features = data.x\n",
        "adj = to_dense_adj(data.edge_index)[0]\n",
        "\n",
        "model.train()\n",
        "for epoch in range(500):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(features, adj)\n",
        "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
        "    \n",
        "    if epoch%10 == 0:\n",
        "        print(loss)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPJUls9KDjBk",
        "outputId": "66f48e18-b04e-491b-8b89-b7e940e5bc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GATBasic(\n",
            "  (attention_0): GATLayer(\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (attention_1): GATLayer(\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (attention_2): GATLayer(\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (attention_3): GATLayer(\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (attention_4): GATLayer(\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (attention_5): GATLayer(\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (attention_6): GATLayer(\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (attention_7): GATLayer(\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv2): GATLayer(\n",
            "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            ")\n",
            "tensor(1.9518, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.8696, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.7932, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.6408, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.5401, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.4727, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.3327, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.3545, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1772, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.1356, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.0620, grad_fn=<NllLossBackward0>)\n",
            "tensor(1.0184, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9814, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9768, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.9390, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8689, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8968, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8952, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8895, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8515, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8575, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8631, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8631, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8073, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8789, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8206, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7088, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7763, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7061, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8442, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8992, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8045, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8313, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8471, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7034, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8364, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7270, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8121, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6650, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7240, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.6885, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7543, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7460, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8659, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7507, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7835, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7493, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7170, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.7455, grad_fn=<NllLossBackward0>)\n",
            "tensor(0.8304, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model prediction for the nodes in the testing set. It should get at least 80% accuracy."
      ],
      "metadata": {
        "id": "foKudEG_FrYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "_, pred = model(features, adj).max(dim=1)\n",
        "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
        "acc = correct / data.test_mask.sum().item()\n",
        "print('Accuracy: {:.4f}'.format(acc))\n",
        "\n",
        "print(pred[data.test_mask])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w442YqmpFvjn",
        "outputId": "7789e92f-515f-4323-e109-e62361f601d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8090\n",
            "tensor([3, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 6, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 6,\n",
            "        2, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5,\n",
            "        5, 5, 2, 2, 2, 2, 2, 6, 6, 1, 0, 0, 0, 0, 5, 0, 0, 0, 3, 0, 0, 6, 0, 0,\n",
            "        1, 3, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 2, 2, 2, 4, 4, 4, 4, 4, 3, 2, 5, 5, 5, 5,\n",
            "        6, 5, 5, 5, 5, 6, 4, 4, 0, 0, 1, 0, 3, 0, 6, 6, 6, 6, 6, 6, 3, 0, 0, 0,\n",
            "        0, 0, 0, 0, 3, 4, 0, 3, 3, 3, 3, 3, 3, 4, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        6, 6, 5, 6, 0, 0, 5, 5, 5, 0, 5, 0, 4, 0, 3, 3, 3, 2, 3, 1, 3, 3, 3, 3,\n",
            "        3, 3, 1, 0, 1, 4, 4, 4, 3, 3, 3, 3, 3, 3, 6, 3, 3, 0, 0, 0, 6, 5, 0, 5,\n",
            "        0, 0, 6, 6, 0, 0, 1, 3, 5, 5, 6, 6, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4,\n",
            "        4, 1, 1, 1, 1, 0, 6, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 5, 5, 3, 3, 3, 3, 3,\n",
            "        0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 3,\n",
            "        1, 1, 1, 1, 1, 1, 0, 0, 6, 6, 0, 0, 3, 0, 1, 1, 0, 6, 6, 6, 6, 2, 3, 3,\n",
            "        0, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 6, 0, 6, 6, 0, 0, 3, 3,\n",
            "        3, 3, 3, 1, 1, 1, 3, 3, 3, 3, 2, 0, 0, 0, 0, 0, 0, 6, 6, 6, 6, 6, 3, 3,\n",
            "        6, 6, 6, 2, 2, 2, 5, 0, 0, 1, 5, 3, 3, 6, 5, 0, 0, 0, 0, 0, 5, 5, 0, 4,\n",
            "        6, 0, 6, 3, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 3, 3, 2, 6, 1, 0,\n",
            "        3, 3, 3, 3, 3, 6, 1, 0, 2, 2, 4, 4, 4, 4, 4, 5, 6, 0, 3, 4, 0, 0, 0, 5,\n",
            "        4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 1, 1, 4, 1, 3, 1, 2, 3, 4,\n",
            "        4, 4, 4, 0, 0, 4, 0, 0, 4, 5, 5, 5, 5, 5, 0, 5, 0, 0, 6, 2, 0, 5, 6, 3,\n",
            "        5, 5, 5, 5, 5, 4, 4, 4, 0, 6, 4, 0, 3, 4, 0, 4, 1, 4, 4, 3, 3, 3, 3, 2,\n",
            "        3, 3, 6, 0, 0, 2, 1, 3, 6, 3, 1, 1, 1, 0, 1, 5, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 2, 4, 4, 4, 3, 1, 3, 3, 0, 3, 3, 4, 4, 0, 4, 4, 4, 4, 4, 4, 4, 4,\n",
            "        0, 4, 4, 2, 3, 3, 4, 5, 0, 2, 2, 3, 3, 3, 3, 3, 2, 3, 5, 5, 4, 1, 4, 4,\n",
            "        4, 4, 4, 4, 0, 3, 4, 4, 6, 2, 2, 2, 2, 4, 6, 6, 6, 0, 3, 4, 4, 4, 3, 3,\n",
            "        0, 5, 3, 5, 0, 3, 3, 3, 3, 2, 2, 2, 0, 4, 0, 0, 3, 2, 6, 0, 0, 6, 0, 0,\n",
            "        5, 1, 3, 4, 4, 4, 4, 4, 6, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2,\n",
            "        2, 6, 6, 6, 5, 6, 6, 3, 2, 6, 4, 4, 4, 4, 2, 5, 5, 0, 0, 3, 0, 4, 4, 3,\n",
            "        2, 3, 1, 6, 6, 5, 0, 4, 4, 6, 3, 1, 1, 4, 0, 5, 2, 3, 4, 3, 4, 5, 4, 0,\n",
            "        3, 3, 0, 2, 1, 1, 5, 2, 3, 3, 5, 0, 2, 3, 2, 2, 5, 5, 4, 3, 4, 3, 1, 2,\n",
            "        4, 2, 4, 5, 5, 3, 2, 3, 2, 3, 3, 3, 4, 5, 4, 3, 3, 3, 1, 3, 0, 1, 2, 4,\n",
            "        4, 4, 3, 3, 3, 5, 2, 3, 2, 2, 2, 3, 2, 2, 6, 4, 4, 2, 0, 0, 3, 4, 3, 3,\n",
            "        3, 3, 3, 0, 0, 4, 3, 3, 3, 2, 3, 4, 2, 2, 5, 4, 3, 4, 4, 1, 5, 2, 4, 3,\n",
            "        2, 2, 1, 3, 2, 3, 6, 3, 6, 3, 0, 2, 6, 1, 3, 0, 2, 3, 2, 4, 2, 4, 1, 2,\n",
            "        0, 5, 6, 4, 3, 3, 3, 2, 5, 3, 5, 2, 3, 3, 3, 3, 3, 4, 6, 2, 4, 2, 2, 2,\n",
            "        5, 4, 4, 4, 4, 6, 1, 2, 2, 6, 2, 2, 2, 2, 2, 3, 4, 4, 4, 3, 3, 4, 3, 3,\n",
            "        3, 0, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 4, 2, 3, 3, 3, 2, 0, 2,\n",
            "        6, 3, 4, 4, 3, 3, 3, 5, 3, 3, 3, 3, 2, 5, 3, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "This demo is based on the skeleton code from the following tutorials:\n",
        "https://colab.research.google.com/github/AntonioLonga/PytorchGeometricTutorial/blob/main/Tutorial3/Tutorial3.ipynb\n",
        "https://colab.research.google.com/drive/1aA--IgJSdPh7J_MIJgs2r0D0VCbGpNTp#scrollTo=Cs42Utl2Hj8O\n",
        "\n",
        "It adds the implementation and testing of the basic GAT network, as long as extra explanations."
      ],
      "metadata": {
        "id": "39pV-XpS7ab5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YhmlBl2-TIEj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "Node Classification with GAT Networks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}